{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dc1dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce356c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearning(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        super(DeepQLearning, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, n_actions)\n",
    "        self.optimizer = optim.SGD(self.parameters(), lr = self.lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9dee6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions, \n",
    "                 max_memory_size = 100000, eps_min = 0.01, eps_dec = 5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.memory_size = max_memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_dec = eps_dec\n",
    "        \n",
    "        self.Q_learn = DeepQLearning(self.lr, input_dims = input_dims, n_actions = n_actions,\n",
    "                                     fc1_dims = 256, fc2_dims = 256)\n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        self.state_memory = np.zeros((self.memory_size,*input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.copy(self.state_memory)\n",
    "        \n",
    "        self.action_memory = np.zeros(self.memory_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.memory_size, dtype=np.float32)\n",
    "        self.condition_memory = np.zeros(self.memory_size, dtype=bool)\n",
    "        \n",
    "    def _store(self, state, action, reward, state_new, done):\n",
    "        idx = self.mem_counter%self.memory_size\n",
    "        self.state_memory[idx] = state\n",
    "        self.action_memory[idx] = action\n",
    "        self.reward_memory[idx] = reward\n",
    "        self.new_state_memory[idx] = state_new\n",
    "        self.condition_memory[idx] = done\n",
    "        \n",
    "        self.mem_counter += 1\n",
    "        \n",
    "    def predict(self, obs):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = torch.tensor([obs]).to(self.Q_learn.device)\n",
    "            act = self.Q_learn.forward(state)\n",
    "            action = torch.argmax(act).item()\n",
    "        else:   \n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.mem_counter < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        self.Q_learn.optimizer.zero_grad()\n",
    "        \n",
    "        max_memory = min(self.mem_counter, self.memory_size)\n",
    "        batch = np.random.choice(max_memory, self.batch_size, replace=False)\n",
    "        batch_idx = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        state_batch = torch.tensor(self.state_memory[batch]).to(self.Q_learn.device)\n",
    "        new_state_batch = torch.tensor(self.new_state_memory[batch]).to(self.Q_learn.device)\n",
    "        reward_batch = torch.tensor(self.reward_memory[batch]).to(self.Q_learn.device)\n",
    "        condition_batch = torch.tensor(self.condition_memory[batch]).to(self.Q_learn.device)\n",
    "        \n",
    "        action_batch = self.action_memory[batch]\n",
    "        \n",
    "        q_eval = self.Q_learn.forward(state_batch)[batch_idx, action_batch]\n",
    "        q_next = self.Q_learn.forward(new_state_batch)\n",
    "        q_next[condition_batch] = 0.0\n",
    "        \n",
    "        q_target = reward_batch + self.gamma*torch.max(q_next, dim=1)[0]\n",
    "        \n",
    "        loss = self.Q_learn.loss(q_target, q_eval).to(self.Q_learn.device)\n",
    "        loss.backward()\n",
    "        self.Q_learn.optimizer.step()\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \\\n",
    "                       else self.eps_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cfc0cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = gym.make('LunarLander-v2')\n",
    "agent = Agent(gamma = 0.99, \n",
    "              epsilon = 1.0, \n",
    "              batch_size = 64, \n",
    "              n_actions = 4, \n",
    "              eps_min = 0.01,\n",
    "              input_dims = [8],\n",
    "              lr = 0.003)\n",
    "n_episode = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4caec4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, score: -262.6716093868959, average score: -262.6716093868959           epsilon: 0.01\n",
      "Episode: 1, score: -78.63229203011721, average score: -170.65195070850655           epsilon: 0.01\n",
      "Episode: 2, score: -101.89039214701998, average score: -147.73143118801102           epsilon: 0.01\n",
      "Episode: 3, score: 15.331178097233305, average score: -106.96577886669994           epsilon: 0.01\n",
      "Episode: 4, score: -54.71834919683396, average score: -96.51629293272674           epsilon: 0.01\n",
      "Episode: 5, score: 2.0993690658067976, average score: -80.08034926630448           epsilon: 0.01\n",
      "Episode: 6, score: 94.0421041253517, average score: -55.205713067496454           epsilon: 0.01\n",
      "Episode: 7, score: -111.30596991720031, average score: -62.21824517370944           epsilon: 0.01\n",
      "Episode: 8, score: -70.89594128625677, average score: -63.18243363065915           epsilon: 0.01\n",
      "Episode: 9, score: -447.0592649324501, average score: -101.57011676083825           epsilon: 0.01\n",
      "Episode: 10, score: 158.71169279190093, average score: -77.9081340742256           epsilon: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m_/x__7ln0x6xv53hn1k5b6b2q00000gn/T/ipykernel_35186/1079182575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m_/x__7ln0x6xv53hn1k5b6b2q00000gn/T/ipykernel_35186/1608819330.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mq_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_learn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mq_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_learn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mq_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcondition_batch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m_/x__7ln0x6xv53hn1k5b6b2q00000gn/T/ipykernel_35186/983096336.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores, epsilon_history = [], []\n",
    "for i in range(n_episode):\n",
    "    score = 0\n",
    "    done = False\n",
    "    obs = train_env.reset()[0]\n",
    "    while not done:\n",
    "        action = agent.predict(obs)\n",
    "        obs_new, reward, done, _, info = train_env.step(action)\n",
    "        score += reward\n",
    "        agent._store(obs, action, reward, obs_new, done)\n",
    "        agent.learn()\n",
    "        obs = obs_new\n",
    "        \n",
    "    scores.append(score)\n",
    "    epsilon_history.append(agent.epsilon)\n",
    "    \n",
    "    avg_score = np.mean(scores)\n",
    "    print(f\"Episode: {i}, score: {score}, average score: {avg_score}\\\n",
    "           epsilon: {agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0170dd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, score: 99.10264916927697\n",
      "Episode: 2, score: 98.72437845937839\n",
      "Episode: 3, score: 99.22752320890349\n",
      "Episode: 4, score: 99.16180215615377\n",
      "Episode: 5, score: 99.09039753788743\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make('LunarLander-v2', render_mode='human')\n",
    "\n",
    "for i in range(5):\n",
    "    obs = test_env.reset()[0]\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        test_env.render()\n",
    "        action = agent.predict(obs)\n",
    "        obs, reward, done, _, info = train_env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "    print(f\"Episode: {i+1}, score: {score}\")\n",
    "    \n",
    "test_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
